I0318 12:02:39.068426 149 pinned_memory_manager.cc:277] "Pinned memory pool is created at '0x7f1318000000' with size 268435456"
I0318 12:02:39.071546 149 cuda_memory_manager.cc:107] "CUDA memory pool is created on device 0 with size 67108864"
I0318 12:02:39.077727 149 model_lifecycle.cc:472] "loading: Primary_Detect:1"
I0318 12:02:39.077777 149 model_lifecycle.cc:472] "loading: Secondary_Classify:1"
I0318 12:02:39.095174 149 tensorrt.cc:65] "TRITONBACKEND_Initialize: tensorrt"
I0318 12:02:39.095228 149 tensorrt.cc:75] "Triton TRITONBACKEND API version: 1.19"
I0318 12:02:39.095237 149 tensorrt.cc:81] "'tensorrt' TRITONBACKEND API version: 1.19"
I0318 12:02:39.095246 149 tensorrt.cc:105] "backend configuration:\n{\"cmdline\":{\"auto-complete-config\":\"false\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"default-max-batch-size\":\"4\"}}"
I0318 12:02:39.103059 149 tensorrt.cc:231] "TRITONBACKEND_ModelInitialize: Secondary_Classify (version 1)"
I0318 12:02:39.103273 149 tensorrt.cc:231] "TRITONBACKEND_ModelInitialize: Primary_Detect (version 1)"
I0318 12:02:39.103983 149 tensorrt.cc:297] "TRITONBACKEND_ModelInstanceInitialize: Secondary_Classify_0_0 (GPU device 0)"
I0318 12:02:39.104002 149 tensorrt.cc:297] "TRITONBACKEND_ModelInstanceInitialize: Primary_Detect_0_0 (GPU device 0)"
I0318 12:02:39.156435 149 logging.cc:46] "Loaded engine size: 38 MiB"
I0318 12:02:39.201722 149 logging.cc:46] "The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value."
I0318 12:02:39.209476 149 logging.cc:46] "[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +43, now: CPU 1, GPU 76 (MiB)"
I0318 12:02:39.209952 149 instance_state.cc:186] "Created instance Secondary_Classify_0_0 on GPU 0 with stream priority 0 and optimization profile default[0];"
I0318 12:02:39.210269 149 model_lifecycle.cc:839] "successfully loaded 'Secondary_Classify'"
I0318 12:02:39.212409 149 logging.cc:46] "Loaded engine size: 8 MiB"
I0318 12:02:39.279241 149 logging.cc:46] "[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +81, now: CPU 2, GPU 162 (MiB)"
I0318 12:02:39.279700 149 instance_state.cc:186] "Created instance Primary_Detect_0_0 on GPU 0 with stream priority 0 and optimization profile default[0];"
I0318 12:02:39.279899 149 tensorrt.cc:297] "TRITONBACKEND_ModelInstanceInitialize: Primary_Detect_0_1 (GPU device 0)"
I0318 12:02:39.292120 149 logging.cc:46] "Loaded engine size: 8 MiB"
I0318 12:02:39.349332 149 logging.cc:46] "[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +81, now: CPU 3, GPU 248 (MiB)"
I0318 12:02:39.349821 149 instance_state.cc:186] "Created instance Primary_Detect_0_1 on GPU 0 with stream priority 0 and optimization profile default[0];"
I0318 12:02:39.349978 149 tensorrt.cc:297] "TRITONBACKEND_ModelInstanceInitialize: Primary_Detect_0_2 (GPU device 0)"
I0318 12:02:39.360025 149 logging.cc:46] "Loaded engine size: 8 MiB"
I0318 12:02:39.415626 149 logging.cc:46] "[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +80, now: CPU 4, GPU 333 (MiB)"
I0318 12:02:39.416125 149 instance_state.cc:186] "Created instance Primary_Detect_0_2 on GPU 0 with stream priority 0 and optimization profile default[0];"
I0318 12:02:39.416286 149 tensorrt.cc:297] "TRITONBACKEND_ModelInstanceInitialize: Primary_Detect_0_3 (GPU device 0)"
I0318 12:02:39.425926 149 logging.cc:46] "Loaded engine size: 8 MiB"
I0318 12:02:39.481912 149 logging.cc:46] "[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +80, now: CPU 5, GPU 419 (MiB)"
I0318 12:02:39.482409 149 instance_state.cc:186] "Created instance Primary_Detect_0_3 on GPU 0 with stream priority 0 and optimization profile default[0];"
I0318 12:02:39.482617 149 model_lifecycle.cc:839] "successfully loaded 'Primary_Detect'"
I0318 12:02:39.482712 149 server.cc:604] 
+------------------+------+
| Repository Agent | Path |
+------------------+------+
+------------------+------+

I0318 12:02:39.482809 149 server.cc:631] 
+----------+-----------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend  | Path                                                      | Config                                                                                                                                                         |
+----------+-----------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tensorrt | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so | {"cmdline":{"auto-complete-config":"false","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}} |
+----------+-----------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0318 12:02:39.482859 149 server.cc:674] 
+--------------------+---------+--------+
| Model              | Version | Status |
+--------------------+---------+--------+
| Primary_Detect     | 1       | READY  |
| Secondary_Classify | 1       | READY  |
+--------------------+---------+--------+

I0318 12:02:39.553636 149 metrics.cc:877] "Collecting metrics for GPU 0: NVIDIA GeForce RTX 3070"
I0318 12:02:39.556505 149 metrics.cc:770] "Collecting CPU metrics"
I0318 12:02:39.556687 149 tritonserver.cc:2598] 
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                           |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                                          |
| server_version                   | 2.49.0                                                                                                                                                                                                          |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics trace logging |
| model_repository_path[0]         | ./triton_model                                                                                                                                                                                                  |
| model_control_mode               | MODE_NONE                                                                                                                                                                                                       |
| strict_model_config              | 1                                                                                                                                                                                                               |
| model_config_name                |                                                                                                                                                                                                                 |
| rate_limit                       | OFF                                                                                                                                                                                                             |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                                       |
| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                                        |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                             |
| strict_readiness                 | 1                                                                                                                                                                                                               |
| exit_timeout                     | 30                                                                                                                                                                                                              |
| cache_enabled                    | 0                                                                                                                                                                                                               |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0318 12:02:39.559432 149 grpc_server.cc:2463] "Started GRPCInferenceService at 0.0.0.0:8001"
I0318 12:02:39.559715 149 http_server.cc:4694] "Started HTTPService at 0.0.0.0:8000"
I0318 12:02:39.600640 149 http_server.cc:362] "Started Metrics Service at 0.0.0.0:8002"
