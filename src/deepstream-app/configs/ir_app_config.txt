[application]
enable-perf-measurement=1
perf-measurement-interval-sec=5
enable-jpeg-save=1

[tiled-display]
enable=0
rows=1
columns=1
width=1280
height=720
gpu-id=0
nvbuf-memory-type=0

[source0]
enable=1
type=4
uri=rtsp://192.168.1.80/live/test
#rtsp://admin:12345yyz@192.168.1.21:554/h264/ch1/main/av_stream
#rtsp://192.168.1.80/live/test
#rtsp://192.168.1.193:25544/live/test
#rtsp://192.168.1.193:25544/live/test
#rtsp://admin:Abc.12345@192.168.1.64/ch1/stream0
num-sources=1
gpu-id=0
cudadec-memtype=0
rtsp-reconnect-interval-sec=3
rtsp-reconnect-attempts=-1
latency=300
select-rtp-protocol=4
smart-record=0
smart-rec-dir-path=/workspace/deepstream-app-custom/smart_rec_ir
smart-rec-duration=20
smart-rec-start-time=5

[sink0]
enable=1
type=4
sync=1
source-id=0
gpu-id=0
nvbuf-memory-type=0
codec=1
bitrate=2000000
iframeinterval=15
rtsp-port=8555
udp-port=5400
width=1280
height=720
profile=4
udp-buffer-size=100000

# redis
[sink1]
enable=0
#Type - 1=FakeSink 2=EglSink 3=File 4=UDPSink 5=nvdrmvideosink 6=MsgConvBroker
type=6
msg-conv-config=dstest5_msgconv_sample_config.txt
#(0): PAYLOAD_DEEPSTREAM - Deepstream schema payload
#(1): PAYLOAD_DEEPSTREAM_MINIMAL - Deepstream schema payload minimal
#(256): PAYLOAD_RESERVED - Reserved type
#(257): PAYLOAD_CUSTOM   - Custom schema payload
msg-conv-payload-type=0
msg-broker-proto-lib=/opt/nvidia/deepstream/deepstream/lib/libnvds_redis_proto.so
#Provide your msg-broker-conn-str here
msg-broker-conn-str=127.0.0.1;6379;dddd
topic=dddd
#Optional:
#msg-broker-config=../../deepstream-test4/cfg_kafka.txt

# 自定义的udp组播
[sink2]
enable=0
#Type - 1=FakeSink 2=EglSink 3=File 4=UDPSink 5=nvdrmvideosink 6=MsgConvBroker 7=MyNetwork
type=7

# mqtt
[sink3]
enable=0
#Type - 1=FakeSink 2=EglSink 3=File 4=UDPSink 5=nvdrmvideosink 6=MsgConvBroker
type=6
msg-conv-config=dstest5_msgconv_sample_config.txt
#(0): PAYLOAD_DEEPSTREAM - Deepstream schema payload
#(1): PAYLOAD_DEEPSTREAM_MINIMAL - Deepstream schema payload minimal
#(256): PAYLOAD_RESERVED - Reserved type
#(257): PAYLOAD_CUSTOM   - Custom schema payload
msg-conv-payload-type=0
msg-broker-proto-lib=/opt/nvidia/deepstream/deepstream/lib/libnvds_mqtt_proto.so
#Provide your msg-broker-conn-str here
msg-broker-conn-str=192.168.1.80;1883
topic=ir_detection
#Optional:
# msg-broker-config=cfg_mqtt.txt

# 配合mqtt
[message-converter]
enable=0
msg-conv-config=dstest5_msgconv_sample_config.txt
#(0): PAYLOAD_DEEPSTREAM - Deepstream schema payload
#(1): PAYLOAD_DEEPSTREAM_MINIMAL - Deepstream schema payload minimal
#(256): PAYLOAD_RESERVED - Reserved type
#(257): PAYLOAD_CUSTOM   - Custom schema payload
msg-conv-payload-type=0
# Name of library having custom implementation.
#msg-conv-msg2p-lib=<val>
# Id of component in case only selected message to parse.
#msg-conv-comp-id=<val>W
msg-conv-msg2p-lib=/workspace/deepstream-app-custom/src/nvmsgconv/libnvds_msgconv.so

# Configure this group to enable cloud message consumer.
[message-consumer]
enable=0
proto-lib=/opt/nvidia/deepstream/deepstream/lib/libnvds_mqtt_proto.so
conn-str=192.168.1.80;1883
# config-file=cfg_mqtt.txt
subscribe-topic-list=command;test2;test3
# Use this option if message has sensor name as id instead of index (0,1,2 etc.).
#sensor-list-file=dstest5_msgconv_sample_config.txt

[osd]
enable=1
gpu-id=0
border-width=5
text-size=15
text-color=1;1;1;1;
text-bg-color=0.3;0.3;0.3;1
font=Serif
show-clock=1
clock-x-offset=100
clock-y-offset=80
clock-text-size=12
clock-color=1;0;0;0
nvbuf-memory-type=0

[streammux]
gpu-id=0
live-source=1
batch-size=1
batched-push-timeout=40000
width=1920
height=1080
nvbuf-memory-type=0
attach-sys-ts-as-ntp=0
enable-padding=1

[pre-process]
enable=1
config-file=config_preprocess_ir_primary.txt

# 直接推理
[primary-gie]
enable=1
gpu-id=0
gie-unique-id=1
#(0): nvinfer - Default inference plugin based on Tensorrt
#(1): nvinferserver - inference plugin based on Tensorrt-Inference-Server
plugin-type=0
nvbuf-memory-type=0
input-tensor-meta=0
# input-tensor-meta=1
config-file=config_infer_primary_yoloV11_ir.txt

# 通过triton调用模型
# [primary-gie]
# enable=1
# gpu-id=0
# #(0): nvinfer - Default inference plugin based on Tensorrt
# #(1): nvinferserver - inference plugin based on Tensorrt-Inference-Server
# plugin-type=1
# batch-size=1
# interval=0
# gie-unique-id=1
# nvbuf-memory-type=0
# config-file=config_infer_triton_yolov11.txt

# [tracker]
# enable=1
# # For NvDCF and NvDeepSORT tracker, tracker-width and tracker-height must be a multiple of 32, respectively
# tracker-width=960
# tracker-height=480
# ll-lib-file=/opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so
# # ll-config-file required to set different tracker types
# # ll-config-file=/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_tracker_IOU.yml
# # ll-config-file=/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_tracker_NvSORT.yml
# ll-config-file=/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_tracker_IOU.yml
# # config_tracker_NvDCF_perf.yml
# # ll-config-file=/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_tracker_NvDCF_accuracy.yml
# # ll-config-file=/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_tracker_NvDeepSORT.yml
# gpu-id=0
# display-tracking-id=1

# 自定义的单目标跟踪
[tracker]
enable=0
# For NvDCF and NvDeepSORT tracker, tracker-width and tracker-height must be a multiple of 32, respectively
tracker-width=1920
tracker-height=1080
ll-lib-file=/opt/nvidia/deepstream/deepstream/lib/libsot.so
ll-config-file=config_sot.yml
gpu-id=0
display-tracking-id=0

# [secondary-pre-process0]
# enable=0
# operate-on-gie-id=1
# config-file=config_preprocess_rgb_sgie.txt

[secondary-gie0]
enable=1
#(0): nvinfer; (1): nvinferserver
plugin-type=0
# Use preprocessed input tensors attached as metadata by nvdspreprocess plugin instead of preprocessing inside the nvinfer.
# input-tensor-meta=1
# nvinferserserver's gpu-id can only set from its own config-file
gpu-id=0
batch-size=4
gie-unique-id=4
operate-on-gie-id=1
# operate-on-class-ids=0;1;2
config-file=config_infer_secondary_ir_classify.txt

# 通过triton调用模型
# [secondary-gie0]
# enable=0
# #(0): nvinfer; (1): nvinferserver
# plugin-type=1
# gpu-id=0
# batch-size=1
# gie-unique-id=4
# operate-on-gie-id=1
# operate-on-class-ids=0;
# config-file=config_infer_secondary_plan_engine.txt

[videorecognition]
enable=0
unique-id=15
gpu-id=0
nvbuf-memory-type=3
processing-width=32
processing-height=32
model-clip-length=8
num-clips=4
# model-type: 0=multi-frame image classification (默认), 1=video recognition
model-type=0
trt-engine-file=/workspace/deepstream-app-custom/src/deepstream-app/models/yolov11m_classify_ir_b4_v2_fp16.engine

# 新增的 UDP Multicast 源配置示例
[udpmulticast]
enable=0
multicast-ip=230.1.1.22
port=8001
interface=enp5s0
# recv-buf-size=1048576

# Configure this group to enable cloud message consumer.
[message-consumer0]
enable=0
proto-lib=/opt/nvidia/deepstream/deepstream/lib/libnvds_mqtt_proto.so
conn-str=<host>;<port>
config-file=<broker config file e.g. cfg_kafka.txt>
subscribe-topic-list=<topic1>;<topic2>;<topicN>
# Use this option if message has sensor name as id instead of index (0,1,2 etc.).
#sensor-list-file=dstest5_msgconv_sample_config.txt
